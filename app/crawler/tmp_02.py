import json

import sys
import os

import yfinance as yf
import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime

def get_sp500_historical_snapshots(years=[2019, 2020, 2021], save_func=None):
    """
    å¾ Wayback Machine æŠ“å– Wikipedia S&P 500 æ­·å²ç‰ˆæœ¬
    æ‰¾å‡ºæ¯å¹´ç¬¬ä¸€å€‹å¿«ç…§ï¼Œå­˜æˆ df å¾Œå‘¼å« save_func
    
    Parameters:
    -----------
    years : list
        è¦æŠ“å–çš„å¹´ä»½åˆ—è¡¨
    save_func : function
        å„²å­˜å‡½æ•¸ï¼Œæ ¼å¼ç‚º save_func(df, folder, filename)
    """
    
    for year in years:
        print(f"\nğŸ” æ­£åœ¨è™•ç† {year} å¹´...")
        
        # æ‰¾å‡ºè©²å¹´ç¬¬ä¸€å€‹å¯ç”¨çš„å¿«ç…§
        snapshot_url = find_first_snapshot(year)
        
        if snapshot_url is None:
            print(f"âŒ æ‰¾ä¸åˆ° {year} å¹´çš„å¿«ç…§")
            continue
        
        print(f"âœ… æ‰¾åˆ°å¿«ç…§: {snapshot_url}")
        
        # æŠ“å–è©²å¿«ç…§çš„è³‡æ–™
        try:
            df = scrape_sp500_from_snapshot(snapshot_url)
            print(f"âœ… æˆåŠŸæŠ“å– {year} å¹´è³‡æ–™ï¼Œå…± {len(df)} ç­†")
            
            # å‘¼å«å„²å­˜å‡½æ•¸
            if save_func:
                save_func(df, "document", f"sp500_{year}")
                print(f"âœ… å·²å„²å­˜ {year} å¹´è³‡æ–™")
            
            # ç¦®è²Œæ€§ç­‰å¾…
            time.sleep(2)
            
        except Exception as e:
            print(f"âŒ æŠ“å– {year} å¹´è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")


def find_first_snapshot(year):
    """æ‰¾å‡ºæŒ‡å®šå¹´ä»½ç¬¬ä¸€å€‹å¯ç”¨çš„ Wayback Machine å¿«ç…§"""
    
    # Wayback Machine CDX API
    cdx_api = "http://web.archive.org/cdx/search/cdx"
    
    params = {
        'url': 'en.wikipedia.org/wiki/List_of_S%26P_500_companies',
        'from': f'{year}0101',
        'to': f'{year}1231',
        'output': 'json',
        'limit': '1',
        'filter': 'statuscode:200'
    }
    
    try:
        response = requests.get(cdx_api, params=params, timeout=30)
        response.raise_for_status()
        data = response.json()
        
        if len(data) > 1:
            timestamp = data[1][1]
            snapshot_url = f"https://web.archive.org/web/{timestamp}/https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
            return snapshot_url
            
    except Exception as e:
        print(f"âš ï¸  API æŸ¥è©¢å¤±æ•—: {e}")
    
    # å‚™ç”¨æ–¹æ³•ï¼šå˜—è©¦å¸¸è¦‹æ—¥æœŸ
    common_dates = [
        f'{year}0101', f'{year}0115', f'{year}0201', 
        f'{year}0301', f'{year}0401', f'{year}0501'
    ]
    
    for date in common_dates:
        test_url = f"https://web.archive.org/web/{date}/https://en.wikipedia.org/wiki/List_of_S%26P_500_companies"
        try:
            response = requests.head(test_url, timeout=10, allow_redirects=True)
            if response.status_code == 200:
                return test_url
        except:
            continue
    
    return None


def scrape_sp500_from_snapshot(snapshot_url):
    """å¾ Wayback Machine å¿«ç…§æŠ“å– S&P 500 è³‡æ–™"""
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    response = requests.get(snapshot_url, headers=headers, timeout=30)
    response.raise_for_status()
    
    soup = BeautifulSoup(response.content, 'html.parser')
    
    # æ‰¾åˆ°ç¬¬ä¸€å€‹è¡¨æ ¼
    table = soup.find('table', {'class': 'wikitable sortable'})
    
    if table is None:
        table = soup.find('table', {'id': 'constituents'})
    
    if table is None:
        raise ValueError("æ‰¾ä¸åˆ° S&P 500 è¡¨æ ¼")
    
    # è§£æè¡¨æ ¼æ¨™é¡Œ
    headers = []
    header_row = table.find('tr')
    for th in header_row.find_all('th'):
        headers.append(th.get_text(strip=True))
    
    # è®€å–è³‡æ–™è¡Œ
    rows = []
    for tr in table.find_all('tr')[1:]:
        cells = tr.find_all(['td', 'th'])
        if len(cells) > 0:
            row = [cell.get_text(strip=True) for cell in cells]
            rows.append(row)
    
    # å»ºç«‹ DataFrame
    df = pd.DataFrame(rows, columns=headers)
    
    # æ¸…ç†è³‡æ–™
    df = df.dropna(how='all')
    df.columns = df.columns.str.strip()
    df = df.reset_index(drop=True)
    
    return df

def save_df_to_json(df, folder_path, filename=None):
    """
    å°‡ DataFrame å„²å­˜ç‚º JSON æª”æ¡ˆï¼Œæª”åæ ¼å¼ç‚ºï¼šå¹´æœˆæ—¥_æ™‚åˆ†ç§’.json

    åƒæ•¸ï¼š
        df (pd.DataFrame): è¦å„²å­˜çš„è³‡æ–™
        folder_path (str): è¦å„²å­˜çš„è³‡æ–™å¤¾è·¯å¾‘
        filename (str, optional): æª”æ¡ˆåç¨±ï¼Œè‹¥ç‚ºç©ºå‰‡ä½¿ç”¨æ™‚é–“æˆ³å‘½å
    """
    # ç¢ºä¿è³‡æ–™å¤¾å­˜åœ¨
    os.makedirs(folder_path, exist_ok=True)

    # ç”Ÿæˆæª”æ¡ˆåç¨±
    if filename is None or filename == "":
        filename = datetime.now().strftime("%Y%m%d_%H%M%S.json")
    elif not filename.endswith('.json'):
        filename = f"{filename}.json"

    filepath = os.path.join(folder_path, filename)

    # å„²å­˜ç‚º JSON
    df.to_json(filepath, orient='records', force_ascii=False, indent=2)
    print(f'æª”æ¡ˆå·²å„²å­˜è‡³ï¼š{filepath}')

def start_point():
    # https://web.archive.org/web/20250415000000*/https://en.wikipedia.org/wiki/List_of_S%26P_500_companies
    get_sp500_historical_snapshots(years=[2014, 2015, 2016, 2017, 2018], save_func=save_df_to_json)

